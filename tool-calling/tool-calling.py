from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, ToolMessage #HumanMessage represents messages from a human user, while ToolMessage is used to convey the results of executing a tool by the AI. 
import requests
from dotenv import load_dotenv
load_dotenv()

# tool create
@tool
def multiply(a: int, b: int) -> int:
  """Given 2 numbers a and b this tool returns their product"""
  return a * b

# print(multiply.invoke({'a':3, 'b':4}))

llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash")
llm_with_tools = llm.bind_tools([multiply])

# print(llm_with_tools.invoke('Hi how are you'))

query = HumanMessage('can you multiply 3 with 1000')
messages = [query]
# print(messages)

result = llm_with_tools.invoke(messages)
messages.append(result)
# print(messages)

tool_result = multiply.invoke(result.tool_calls[0]["args"]) # This is the result of the tool call, here we are calling the multiply function. the second parameter is the ['args']. this helps us to get the arguments from the tool call

print(tool_result)

tool_call_id = result.tool_calls[0]["id"] # This is the id of the tool call, which is used to identify the specific tool call in the conversation history. The tool_call_id is used to create a ToolMessage object, which represents the result of executing the tool.
tool_message = ToolMessage(content=str(tool_result), tool_call_id=tool_call_id) # This is the result of the tool call, which is used to create a ToolMessage object. The ToolMessage object is then appended to the messages list, which contains all the messages exchanged in the conversation.
messages.append(tool_message)

print(llm_with_tools.invoke(messages).content) # This is the final result of the conversation, which is generated by the LLM based on the messages exchanged so far. The final result is printed to the console.
