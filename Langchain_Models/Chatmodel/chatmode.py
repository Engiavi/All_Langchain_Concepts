from langchain_openai import ChatOpenAI
# ChatOpenAI is inherited from BaseChatModel
# while OpenAI is inherited from BaseLLM
from dotenv import load_dotenv
load_dotenv()

chatModel=ChatOpenAI(model="gpt-4",temperture=1.5, max_tokens=1000, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0)
# here model = "gpt-4" is the model name, you can use any model name from the list of models
# temperture is the parameter for the model, which lie between 0 and 2, it helps to control the randomness of the model, default is 0.7
# max_tokens is the maximum number of tokens to generate, default is 100 it limits the response length, which is generated by the model
# top_p is the parameter for nucleus sampling, default is 1.0, nucleus sampling is a method to generate the response, it is different from temperature. as it is used to control the randomness of the model while temperature helps in controlling the randomness of the model
# frequency_penalty is the parameter to control the frequency of words, default is 0.0, it is basically use in the case of chat model, to control the frequency of words.
# presence_penalty is the parameter to control the presence of words, default is 0.0, it is basically use in the case of chat model, to control the presence of words.

result = chatModel.invoke("what is the capital of India?")

print(result)

# in LLM code we get only the string says "New Delhi", but here we get the whole response from the model
# like content="the capital of India is New Delhi", additionally it contains various other information
